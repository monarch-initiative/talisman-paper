{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4327d3f7",
   "metadata": {},
   "source": [
    "# Preprocess\n",
    "\n",
    "Preprocess the raw results from OntoGPT\n",
    "\n",
    "Draft: https://docs.google.com/document/d/1H103ux6Dd1_bPM0un4RwutBLcYJx-0ybil2AwlAvG_Q/edit#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9fa1d",
   "metadata": {},
   "source": [
    "## Initial setup\n",
    "\n",
    "Import libraries, create a GO adapter (for calculating closures/ancestors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3452b4b0",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# note the gpt4 dir includes combined results from davinci, 3.5 and 4\n",
    "results_dir = \"results/human/gpt4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97e39d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from yaml import Loader\n",
    "from collections import defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from oaklib import get_adapter\n",
    "from oaklib.datamodels.vocabulary import IS_A, PART_OF\n",
    "from ontogpt.evaluation.enrichment.eval_enrichment import EvalEnrichment\n",
    "go = get_adapter(\"sqlite:obo:go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073f0423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the closure/ancestor map\n",
    "# (takes a minute)\n",
    "closure_map = defaultdict(set)\n",
    "for s, _, o in go.relationships(predicates=[IS_A, PART_OF], include_entailed=True):\n",
    "    closure_map[s].add(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17160aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84220\n"
     ]
    }
   ],
   "source": [
    "print(len(closure_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44781ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ruamel is faster than pyyaml\n",
    "from ruamel.yaml import YAML\n",
    "ryaml = YAML()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe25eb0-1619-4ab5-87fe-a0593a232eb6",
   "metadata": {},
   "source": [
    "## Load YAML results\n",
    "\n",
    "See OntoGPT for details of data model\n",
    "\n",
    "These results are nested, with a parent gene set object containing all runs.\n",
    "We want to flatten this for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e16189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ontogpt.evaluation.enrichment.eval_enrichment import GeneSetComparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca76b18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assumes comparisons have been run and concatenated (see Makefile) \n",
    "import glob\n",
    "def load_gene_set_results():\n",
    "    results = []\n",
    "    for fn in glob.glob(f\"{results_dir}/*.yaml\"):\n",
    "        print(fn)\n",
    "        with open(fn) as f:\n",
    "            #obj = yaml.load(f, Loader)\n",
    "            #obj = yaml.safe_load(f)\n",
    "            obj = ryaml.load(f)\n",
    "            results.extend(obj)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebab4da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results/human/gpt4/canonical-glycolysis-gocam-results-2.yaml\n",
      "results/human/gpt4/bicluster_RNAseqDB_1001-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_HYPOXIA-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_DNA_REPAIR-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_G2M_CHECKPOINT-results-2.yaml\n",
      "results/human/gpt4/EDS-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_IL2_STAT5_SIGNALING-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_EPITHELIAL_MESENCHYMAL_TRANSITION-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_PI3K_AKT_MTOR_SIGNALING-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_COAGULATION-results-2.yaml\n",
      "results/human/gpt4/peroxisome-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_APICAL_JUNCTION-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_ANGIOGENESIS-results-2.yaml\n",
      "results/human/gpt4/go-postsynapse-calcium-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_BILE_ACID_METABOLISM-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_CHOLESTEROL_HOMEOSTASIS-results-2.yaml\n",
      "results/human/gpt4/bicluster_RNAseqDB_1002-results-2.yaml\n",
      "results/human/gpt4/HALLMARK_KRAS_SIGNALING_DN-results-2.yaml\n"
     ]
    }
   ],
   "source": [
    "comps = load_gene_set_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6422c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(comps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2865fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_by_gene_sets = comps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bd6929",
   "metadata": {},
   "source": [
    "## Create the main data frame\n",
    "\n",
    "For each gene set, we will create a list of rows,\n",
    "for each combination of method + parameters, plus a cutoff.\n",
    "\n",
    "A cutoff of zero means we only consider the *top rank* GO term from enrichment.\n",
    "If the results contain this, it's a true positive; otherwise we count as a single false positive.\n",
    "\n",
    "Otherwise we look at all enrichment results with p-val less than the cutoff. If predicted terms match these,\n",
    "it's a true positive.\n",
    "\n",
    "For calculating false negatives, we check both ancestors and descendants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea7082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "pairs = list(itertools.product([0.005, 0.05, 99], [False, True], [1, 5, 10, 25, 5000]))\n",
    "\n",
    "def eval_gene_set_result(gs):\n",
    "    \"\"\"\n",
    "    For each gene set makes, sets of rows, for each combination of method/params,\n",
    "    plus each combination of p-value cutoff\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    std = gs[\"payloads\"][\"standard\"]\n",
    "    if \"enrichment_results\" not in std:\n",
    "        print(f\"NO GOLD STANDARD: {gs['name']}\")\n",
    "    expected_results = std.get(\"enrichment_results\", [])\n",
    "    for run, method_result in gs[\"payloads\"].items():\n",
    "        predicted_term_ids = method_result.get(\"term_ids\", [])\n",
    "\n",
    "        #closure_map = defaultdict(list)\n",
    "        predicted_term_closure_ids = set()\n",
    "        for t in predicted_term_ids:\n",
    "            predicted_term_closure_ids.update(closure_map.get(t, set()))\n",
    "        #for s, _, o in go.relationships(subjects=predicted_term_ids, predicates=[IS_A, PART_OF], include_entailed=True):\n",
    "            #closure_map[s].append(o)\n",
    "        #    predicted_term_closure_ids.add(o)\n",
    "        for cutoff, use_closure, top_n in pairs:\n",
    "            #if top_n == 1 and cutoff != 0.05:\n",
    "            #    continue\n",
    "            row = {}\n",
    "\n",
    "            ###previously this was from:\n",
    "            ##go_term_ids = [t for t in payload.term_ids if t.startswith(\"GO:\")]\n",
    "            row['go_term_ids'] = predicted_term_closure_ids#gs[\"payloads\"][\"closure\"][\"term_ids\"]#go_term_ids\n",
    "            row[\"name\"] = f\"{gs['name']}-{cutoff}\"\n",
    "            row[\"cutoff\"] = cutoff\n",
    "            row[\"closure\"] = use_closure\n",
    "            row[\"top_n\"] = top_n\n",
    "            method = method_result.get(\"method\", \"\")\n",
    "            approach = \"gpt\"\n",
    "            if method == \"no_synopsis\":\n",
    "                src = \"NONE\"\n",
    "            elif method == \"ontological_synopsis\":\n",
    "                src = \"GO\"\n",
    "            elif method == \"narrative_synopsis\":\n",
    "                src = \"RefSeq\"\n",
    "            else:\n",
    "                approach = method\n",
    "                src = \"\"\n",
    "            row[\"source\"] = src\n",
    "            model = method_result.get(\"model\", \"\")\n",
    "            if model == \"gpt-4\":\n",
    "                model = \"4\"\n",
    "            elif model == \"gpt-3.5-turbo\":\n",
    "                model = \"3.5\"\n",
    "            elif model == \"text-davinci-003\":\n",
    "                model = \"3\"\n",
    "            row[\"model\"] = model\n",
    "            row[\"method\"] = approach\n",
    "            if model:\n",
    "                row[\"method_desc\"] = f\"{method}-{model}\"\n",
    "            else:\n",
    "                row[\"method_desc\"] = method_result.get(\"truncation_factor\", \"\")\n",
    "            row[\"run\"] = run\n",
    "            for k in [\"truncation_factor\", \"prompt_variant\", \"response_token_length\"]:\n",
    "                row[k] = method_result.get(k, \"\")\n",
    "            row[\"prompt_length\"] = len(method_result.get(\"prompt\", \"\"))\n",
    "            true_positive_terms = []\n",
    "            false_negative_terms = []\n",
    "            more_specific_false_negative_terms = []   # predicted a descendant\n",
    "            more_general_false_negative_terms = []   # predicted a descendant\n",
    "            unparsed_terms = []\n",
    "            standard_enrichment_results = [(r[\"p_value_adjusted\"], r[\"class_id\"]) for r in expected_results]\n",
    "            enrichment_closure = set()\n",
    "            enrichment_term_ids = set()\n",
    "            if cutoff == 99:\n",
    "                # extend the enrichment results but with max cutoff\n",
    "                for x in gs[\"payloads\"][\"closure\"][\"term_ids\"]:\n",
    "                    standard_enrichment_results.append((cutoff, x))\n",
    "            visited_terms = set()\n",
    "            n = 0\n",
    "            for p_val, true_term_id in standard_enrichment_results:\n",
    "                if cutoff > 0 and p_val > cutoff:\n",
    "                    break\n",
    "                if true_term_id in visited_terms:\n",
    "                    continue\n",
    "                n += 1\n",
    "                if n > top_n:\n",
    "                    break\n",
    "                enrichment_closure.update(closure_map.get(true_term_id, set()))\n",
    "                enrichment_term_ids.add(true_term_id)\n",
    "                true_term_closure_ids = closure_map.get(true_term_id, set())\n",
    "                if use_closure:\n",
    "                    visited_terms.update(true_term_closure_ids)\n",
    "                else:\n",
    "                    visited_terms.add(true_term_id)\n",
    "                if true_term_id in predicted_term_ids:\n",
    "                    true_positive_terms.append(true_term_id)\n",
    "                elif true_term_id in predicted_term_closure_ids:\n",
    "                    # predicted a more specific term\n",
    "                    if use_closure:\n",
    "                        true_positive_terms.append(true_term_id)\n",
    "                    else:\n",
    "                        false_negative_terms.append(true_term_id)\n",
    "                    more_specific_false_negative_terms.append(true_term_id)\n",
    "                elif true_term_closure_ids.intersection(predicted_term_ids):\n",
    "                    # predicted a more general term\n",
    "                    if use_closure:\n",
    "                        true_positive_terms.append(true_term_id)\n",
    "                    else:\n",
    "                        false_negative_terms.append(true_term_id)\n",
    "                    more_general_false_negative_terms.append(true_term_id)\n",
    "                else:\n",
    "                    false_negative_terms.append(true_term_id)\n",
    "                if cutoff == 0:\n",
    "                    break\n",
    "            false_positive_terms = []\n",
    "            if top_n > 1:\n",
    "                for t in predicted_term_ids:\n",
    "                    if t not in true_positive_terms:\n",
    "                        if t.startswith(\"GO:\"):\n",
    "                            if not use_closure:\n",
    "                                false_positive_terms.append(t)\n",
    "                            else:\n",
    "                                if t in enrichment_closure:\n",
    "                                    # prediction is more general\n",
    "                                    pass\n",
    "                                elif closure_map.get(t, set()).intersection(enrichment_term_ids):\n",
    "                                    # prediction is more specific\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    false_positive_terms.append(t)\n",
    "                        elif \":\" in t:\n",
    "                            # MONDO, UBERON, etc\n",
    "                            pass\n",
    "                        else:\n",
    "                            unparsed_terms.append(t)\n",
    "            else:\n",
    "                if not true_positive_terms:\n",
    "                    if predicted_term_ids:\n",
    "                        false_positive_terms.append(predicted_term_ids[0])\n",
    "            row[\"true_positives\"] = len(true_positive_terms)\n",
    "            row[\"false_positives\"] = len(false_positive_terms)\n",
    "            row[\"false_negatives\"] = len(false_negative_terms)\n",
    "            row[\"more_general_false_negatives\"] = len(more_general_false_negative_terms)\n",
    "            row[\"more_specific_false_negatives\"] = len(more_specific_false_negative_terms)\n",
    "            row[\"all_predictions_closure\"] = len(predicted_term_closure_ids)\n",
    "            row[\"unparsed\"] = len(unparsed_terms)\n",
    "            row[\"true_positive_terms\"] = \"|\".join(true_positive_terms)\n",
    "            row[\"false_positive_terms\"] = \"|\".join(false_positive_terms)\n",
    "            row[\"false_negative_terms_example20\"] = \"|\".join(false_negative_terms)[0:20]\n",
    "            row[\"unparsed_terms\"] = \"|\".join(unparsed_terms)\n",
    "            row[\"gene_set_size\"] = len(gs.get(\"gene_symbols\"))\n",
    "            \n",
    "            denom1 = row[\"true_positives\"] + row[\"false_positives\"]\n",
    "            if denom1 != 0:\n",
    "                row[\"precision\"] = row[\"true_positives\"] / denom1\n",
    "            else:\n",
    "                row[\"precision\"] = 0\n",
    "\n",
    "            denom2 = row[\"true_positives\"] + row[\"false_negatives\"]\n",
    "            if denom2 != 0:\n",
    "                row[\"recall\"] = row[\"true_positives\"] / denom2\n",
    "            else:\n",
    "                row[\"recall\"] = 0\n",
    "                \n",
    "            denom3 = row[\"true_positives\"] + row[\"more_general_false_negatives\"]\n",
    "            if denom3 != 0:\n",
    "                row[\"recall_general\"] = row[\"true_positives\"] / denom3\n",
    "            else:\n",
    "                row[\"recall_general\"] = 0\n",
    "                \n",
    "            denom4 = row[\"true_positives\"] + row[\"more_specific_false_negatives\"]\n",
    "            if denom4 != 0:\n",
    "                row[\"recall_specific\"] = row[\"true_positives\"] / denom4\n",
    "            else:\n",
    "                row[\"recall_specific\"] = 0\n",
    "            \n",
    "            f1_denom = row[\"true_positives\"] + 0.5* (row[\"false_positives\"] + row[\"false_negatives\"]) \n",
    "            if(f1_denom != 0):\n",
    "                row[\"f1_score\"] = row[\"true_positives\"] / (f1_denom)\n",
    "            else:\n",
    "                row[\"f1_score\"] = None\n",
    "\n",
    "            #mcc_denom = math.sqrt((row[\"true_positives\"] + row[\"false_positives\"]) * (row[\"true_positives\"] + row[\"false_negatives\"]) * (row[\"true_negatives\"] + row[\"false_positives\"]) * (row[\"true_negatives\"] + row[\"false_negatives\"]))\n",
    "            #mcc_num = (row[\"true_negatives\"] * row[\"true_positives\"]) -  (row[\"false_negatives\"] * row[\"false_positives\"])\n",
    "            #if(f1_denom != 0):\n",
    "            #    row[\"mcc\"] =  mcc_num / (mcc_denom)\n",
    "            #else:\n",
    "            #    row[\"mcc\"] = None\n",
    "\n",
    "            #row[\"gene_symbols\"] = \"|\".join(gs.get(\"gene_symbols\", []))\n",
    "            #row[\"gene_ids\"] = \"|\".join(gs.get(\"gene_ids\", []))\n",
    "            rows.append(row)\n",
    "    return rows\n",
    "\n",
    "rows = eval_gene_set_result(results_by_gene_sets[4])\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"results/TEST-processed.tsv\", sep=\"\\t\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a55b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for gs in results_by_gene_sets:\n",
    "    this_rows = eval_gene_set_result(gs)\n",
    "    rows.extend(this_rows)\n",
    "    print(len(rows))\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(\"results/TEMP.tsv\", sep=\"\\t\", index=False)\n",
    "    \n",
    "    #if len(rows) % 10 == 0:\n",
    "    #    print(f\"Processed {len(rows)})\n",
    "              \n",
    "df = pd.DataFrame(rows)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "df.to_csv(\"results/processed.tsv\", sep=\"\\t\", index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8106914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "#df[[model, method]].drop_duplicates()\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b79b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"method\"].unique()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
